package test.resources.test_jobs; import org.apache.spark.SparkConf; import java.util.stream.Stream; import org.apache.spark.api.java.JavaSparkContext; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import java.util.AbstractMap.SimpleEntry; import java.util.Arrays; public class SparkJavaWordCount2Java8Translated { private static final Logger LOGGER = LoggerFactory.getLogger(SparkJavaWordCount2Java8Translated.class); public static void main(String[] args) { String master = "local[*]"; SparkConf conf = new SparkConf().setAppName(SparkJavaWordCount2Java8Translated.class.getName()).setMaster(master); JavaSparkContext context = new JavaSparkContext(conf); Stream<String> textFile = context.textFile("swift2d://data1.lvm/hamlet.txt"); textFile.flatMap(s -> Arrays.stream(s.split(" "))).map(word -> new SimpleEntry<String, Integer>(word, 1)).collect(java.util.stream.Collectors.groupingBy(SimpleEntry<String, Integer>::getKey, java.util.stream.Collectors.counting())).forEach(result -> LOGGER.info(String.format("Word [%s] count [%d].", result._1(), result._2()))); } }
